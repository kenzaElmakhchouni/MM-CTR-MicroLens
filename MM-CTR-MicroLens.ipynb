{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13875146,"sourceType":"datasetVersion","datasetId":8820441}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Multimodal CTR Prediction (MM-CTR)\n\nThis notebook presents a complete end-to-end solution for the MM-CTR Challenge, covering both multimodal item embedding (Task 1) and multimodal CTR prediction (Task 2).\n","metadata":{}},{"cell_type":"markdown","source":"# 1. Introduction\n\nThis project addresses the Multimodal Click-through Rate (MM-CTR) Prediction Challenge at WWW 2025 EReL@MIR. The goal is to leverage multimodal item informationâ€”text and imagesâ€”to improve click-through rate prediction under low-latency constraints. We implement the complete pipeline covering Task 1: Multimodal Item Embedding and Task 2: Multimodal CTR Prediction in a single, reproducible notebook.","metadata":{}},{"cell_type":"code","source":"# Force a compatible protobuf version\n# Some libraries (CLIP, sentence-transformers) may break with newer versions\n!pip uninstall -y protobuf\n!pip install protobuf==3.20.3","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Install OpenAI CLIP directly from GitHub\n# Kaggle does not provide it by default\n!pip install git+https://github.com/openai/CLIP.git","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Additional libraries for training & evaluation\n!pip install -q polars tqdm","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==========================\n# Core imports\n# ==========================\nimport os\nimport warnings\n\nimport numpy as np\nimport pandas as pd\nimport torch\n\nfrom PIL import Image\nimport clip\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.decomposition import PCA\n\nimport polars as pl\nimport torch.nn as nn\n\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchmetrics.classification import BinaryAUROC\nfrom tqdm import tqdm\n\n\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 2. Dataset Overview\n\nWe use the MicroLens-1M dataset, which contains user click interactions and rich item content features such as titles and cover images. Public data are used for training and validation, while private data are reserved for testing.","metadata":{}},{"cell_type":"code","source":"# ==========================\n# Dataset paths (Kaggle)\n# ==========================\nINFO_FILE = \"/kaggle/input/item_info.parquet\"\nFEATURE_FILE = \"/kaggle/input/item_feature.parquet\"\nIMAGE_DIR = \"/kaggle/input/item_images/item_images/\"\n\nSAVE_PATH = \"/kaggle/working/item_info_multimodal_emb.parquet\"\n\n# ==========================\n# Read parquet files\n# ==========================\ndf_info = pd.read_parquet(INFO_FILE)\ndf_features = pd.read_parquet(FEATURE_FILE)\n\nprint(\"Item info shape :\", df_info.shape)\nprint(\"Item feature shape :\", df_features.shape)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 3. Task 1: Multimodal Item Embedding\nIn this section, we extract textual and visual representations for each item, then fuse them into a compact 128-dimensional embedding.\n","metadata":{}},{"cell_type":"markdown","source":"# 3.1 Text Embedding and Image Embedding\n\nWe encode item textual information using a pretrained sentence-level encoder to obtain dense semantic representations.\n\nItem cover images are processed using a pretrained vision model to extract visual embeddings.","metadata":{}},{"cell_type":"code","source":"# ==========================\n# Device configuration\n# ==========================\ncompute_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# ==========================\n# Text encoder (Sentence-BERT)\n# ==========================\ntext_encoder = SentenceTransformer(\"all-MiniLM-L6-v2\")\ntext_encoder.eval()\n\n# ==========================\n# Image encoder (CLIP ResNet-50)\n# ==========================\nclip_net, clip_preprocess = clip.load(\"RN50\", device=compute_device)\nclip_net.eval()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==========================\n# Text embeddings\n# ==========================\nitem_titles = df_features[\"item_title\"].tolist()\n\ntext_vectors = text_encoder.encode(\n    item_titles,\n    batch_size=256,\n    show_progress_bar=True\n)\n\n# ==========================\n# Image embeddings \n# ==========================\ndef extract_image_embedding(image_path):\n    \"\"\"\n    Compute CLIP embedding for a single image.\n    If the image is missing or corrupted, return a zero vector.\n    \"\"\"\n    try:\n        img = Image.open(image_path).convert(\"RGB\")\n        img_tensor = clip_preprocess(img).unsqueeze(0).to(compute_device)\n\n        with torch.no_grad():\n            img_emb = clip_net.encode_image(img_tensor)\n\n        return img_emb.cpu().numpy().squeeze()\n\n    except Exception:\n        return np.zeros(clip_net.visual.output_dim)\n\n\n# ==========================\n# Encode all item images\n# ==========================\nimage_vectors = []\n\nfor iid in df_features[\"item_id\"]:\n    img_path = os.path.join(IMAGE_DIR, f\"image{iid}.jpg\")\n    image_vectors.append(extract_image_embedding(img_path))\n\nimage_vectors = np.array(image_vectors)\n\nprint(\"Image embedding matrix shape:\", image_vectors.shape)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Inspect available columns\nprint(df_info.columns.tolist())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 3.3 Multimodal Fusion and Dimensionality Reduction\n\nText and image embeddings are concatenated to form multimodal representations. To satisfy competition constraints and reduce inference latency, PCA is applied to project embeddings to 128 dimensions.","metadata":{}},{"cell_type":"code","source":"from IPython.display import FileLink\n\n# ==========================\n# Multimodal fusion (text + image)\n# ==========================\nmultimodal_features = np.concatenate(\n    [text_vectors, image_vectors],\n    axis=1\n)\n\nprint(\"Fused embedding shape:\", multimodal_features.shape)\n\n# ==========================\n# Padding first row (alignment with item_info)\n# ==========================\nzero_padding = np.zeros((1, multimodal_features.shape[1]))\nmultimodal_features_padded = np.vstack(\n    [zero_padding, multimodal_features]\n)\n\nprint(\"After padding:\", multimodal_features_padded.shape)\n\n# ==========================\n# Dimensionality reduction (PCA â†’ 128)\n# ==========================\npca_model = PCA(n_components=128)\nmultimodal_128d = pca_model.fit_transform(multimodal_features_padded)\n\nprint(\"Final embedding shape:\", multimodal_128d.shape)\n\n# ==========================\n# Save into item_info\n# ==========================\ndf_info[\"item_emb_d128\"] = list(multimodal_128d)\ndf_info.to_parquet(SAVE_PATH, index=False)\n#You can download the saved file from /kaggle/working/ directory\nprint(f\"Saved fused embeddings to: {SAVE_PATH}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Reload saved file for sanity check\ndf_check = pd.read_parquet(SAVE_PATH)\n\nprint(df_check.head())\nprint(\"Embedding dimension example:\", len(df_check[\"item_emb_d128\"].iloc[0]))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 4. Task 2: Multimodal CTR Prediction\n\nIn this task, we focus on predicting the click-through rate (CTR) by effectively leveraging the multimodal item embeddings generated in Task 1. The objective is to design an end-to-end neural CTR model that integrates user interaction features with multimodal item representations.\n\nThe model takes as input user identifiers, target items, historical interaction sequences, and engagement signals (likes and views), together with the 128-dimensional multimodal item embeddings. These features are jointly learned through embedding layers and a multilayer perceptron (MLP) to estimate the probability of a user clicking on a given item.\n\nThe model is trained using binary cross-entropy loss and evaluated using the Area Under the ROC Curve (AUC), following the official data split and competition constraints.\n","metadata":{}},{"cell_type":"markdown","source":"# 4.1 Problem Formulation\n\nGiven user interaction history and item multimodal embeddings, the task is to predict the probability of a click (pCTR; I named it Task1&2 in the submission.csv as you'll find at the end of this notebook) for each userâ€“item pair.","metadata":{}},{"cell_type":"code","source":"# ==========================\n# Kaggle dataset paths\n# ==========================\nBASE_PATH = \"/kaggle/input/\"  \n\ntrain_df = pl.read_parquet(f\"{BASE_PATH}/train.parquet\")\nvalid_df = pl.read_parquet(f\"{BASE_PATH}/valid.parquet\")\ntest_df  = pl.read_parquet(f\"{BASE_PATH}/test.parquet\")\n\n# Load item_info WITH multimodal embeddings\nitem_info = pl.read_parquet(\n    \"/kaggle/working/item_info_multimodal_emb.parquet\"\n)\n\nprint(\n    \"Train / Valid / Test shapes:\",\n    train_df.shape, valid_df.shape, test_df.shape\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==========================\n# Device configuration\n# ==========================\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Using device:\", device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==========================\n# Build item embedding tensor (128-d multimodal)\n# ==========================\nitem_emb_matrix = np.stack(\n    item_info[\"item_emb_d128\"].to_numpy()\n)\n\nitem_emb_tensor = torch.tensor(\n    item_emb_matrix,\n    dtype=torch.float32,\n    device=device\n)\n\nnum_items = item_emb_tensor.shape[0]\nprint(\"Number of items (including padding):\", num_items)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 4.2 Model Architecture\n\nWe design an end-to-end neural CTR model that ingests user features and item multimodal embeddings. The model outputs a scalar pCTR via fully connected layers.","metadata":{}},{"cell_type":"code","source":"# ==========================\n# CTR Dataset\n# ==========================\nclass CTRDataset(Dataset):\n    \"\"\"\n    PyTorch Dataset for CTR prediction.\n    Handles both train/validation and test modes.\n    \"\"\"\n\n    def __init__(self, df, is_test=False):\n        self.is_test = is_test\n\n        self.user_id  = df[\"user_id\"].to_numpy()\n        self.item_id  = df[\"item_id\"].to_numpy()\n        self.item_seq = df[\"item_seq\"].to_numpy()\n        self.likes    = df[\"likes_level\"].to_numpy()\n        self.views    = df[\"views_level\"].to_numpy()\n\n        if not is_test:\n            self.label = df[\"label\"].to_numpy()\n        else:\n            self.ID = df[\"ID\"].to_numpy()\n\n    def __len__(self):\n        return len(self.user_id)\n\n    def __getitem__(self, idx):\n        sample = {\n            \"user_id\": torch.tensor(self.user_id[idx], dtype=torch.long),\n            \"item_id\": torch.tensor(self.item_id[idx], dtype=torch.long),\n            \"item_seq\": torch.tensor(self.item_seq[idx], dtype=torch.long),\n            \"likes\": torch.tensor(self.likes[idx], dtype=torch.long),\n            \"views\": torch.tensor(self.views[idx], dtype=torch.long),\n        }\n\n        if self.is_test:\n            sample[\"ID\"] = torch.tensor(self.ID[idx], dtype=torch.long)\n        else:\n            sample[\"label\"] = torch.tensor(self.label[idx], dtype=torch.float32)\n\n        return sample\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==========================\n# DataLoaders \n# ==========================\nBATCH_SIZE = 1024\n\ntrain_loader = DataLoader(\n    CTRDataset(train_df),\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    num_workers=0,      # IMPORTANT for Kaggle users\n    pin_memory=False    # IMPORTANT for Kaggle users\n)\n\nvalid_loader = DataLoader(\n    CTRDataset(valid_df),\n    batch_size=BATCH_SIZE,\n    shuffle=False,\n    num_workers=0,\n    pin_memory=False\n)\n\ntest_loader = DataLoader(\n    CTRDataset(test_df, is_test=True),\n    batch_size=BATCH_SIZE,\n    shuffle=False,\n    num_workers=0,\n    pin_memory=False\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==========================\n# MMCTR Model\n# ==========================\nclass MMCTRModel(nn.Module):\n    \"\"\"\n    Multimodal CTR model combining:\n    - User embedding\n    - Item embedding\n    - Sequential item history\n    - Context features (likes, views)\n    - Precomputed multimodal item embeddings (CLIP + SBERT)\n    \"\"\"\n\n    def __init__(self, num_users, num_items, emb_dim=32):\n        super().__init__()\n\n        self.user_emb = nn.Embedding(num_users + 1, emb_dim, padding_idx=0)\n        self.item_emb = nn.Embedding(num_items, emb_dim, padding_idx=0)\n\n        self.likes_emb = nn.Embedding(11, emb_dim)\n        self.views_emb = nn.Embedding(11, emb_dim)\n\n        self.mlp = nn.Sequential(\n            nn.Linear(emb_dim * 5 + 128, 256),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Linear(128, 1)\n        )\n\n    def forward(self, user_id, item_id, item_seq, likes, views):\n        user_e = self.user_emb(user_id)\n        item_e = self.item_emb(item_id)\n\n        # Sequence pooling (masked mean)\n        seq_e = self.item_emb(item_seq)\n        mask = (item_seq != 0).unsqueeze(-1)\n        seq_e = (seq_e * mask).sum(1) / (mask.sum(1) + 1e-8)\n\n        likes_e = self.likes_emb(likes)\n        views_e = self.views_emb(views)\n\n        # Multimodal item embedding (frozen)\n        item_mm_e = item_emb_tensor[item_id]\n\n        x = torch.cat(\n            [user_e, item_e, seq_e, likes_e, views_e, item_mm_e],\n            dim=1\n        )\n\n        return self.mlp(x).squeeze(1)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==========================\n# Model setup\n# ==========================\nnum_users = int(train_df[\"user_id\"].max())\n\nmodel = MMCTRModel(\n    num_users=num_users,\n    num_items=num_items\n).to(device)\n\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\nauc_metric = BinaryAUROC().to(device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 4.3 Model Training and Validation\n\nWe train the multimodal CTR model on the training set and monitor its performance on the validation set using AUC. Early stopping is applied to prevent overfitting, and the best-performing model is saved for final inference.\n","metadata":{}},{"cell_type":"code","source":"# ==========================\n# Training loop\n# ==========================\nEPOCHS = 20\nbest_valid_auc = 0.0\npatience = 3\npatience_counter = 0\n\nMODEL_PATH = \"/kaggle/working/best_mmctr_model.pt\"\n\nfor epoch in range(EPOCHS):\n\n    # ===== TRAIN =====\n    model.train()\n    epoch_loss = 0.0\n\n    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n        optimizer.zero_grad()\n\n        logits = model(\n            batch[\"user_id\"].to(device),\n            batch[\"item_id\"].to(device),\n            batch[\"item_seq\"].to(device),\n            batch[\"likes\"].to(device),\n            batch[\"views\"].to(device)\n        )\n\n        loss = criterion(logits, batch[\"label\"].to(device))\n        loss.backward()\n        optimizer.step()\n\n        epoch_loss += loss.item()\n\n    print(f\"Epoch {epoch+1} | Train Loss: {epoch_loss / len(train_loader):.4f}\")\n\n    # ===== VALID =====\n    model.eval()\n    auc_metric.reset()\n\n    with torch.no_grad():\n        for batch in valid_loader:\n            logits = model(\n                batch[\"user_id\"].to(device),\n                batch[\"item_id\"].to(device),\n                batch[\"item_seq\"].to(device),\n                batch[\"likes\"].to(device),\n                batch[\"views\"].to(device)\n            )\n            probs = torch.sigmoid(logits)\n            auc_metric.update(probs, batch[\"label\"].to(device))\n\n    valid_auc = auc_metric.compute().item()\n    print(f\"Epoch {epoch+1} | Valid AUC: {valid_auc:.4f}\")\n\n    # ===== EARLY STOPPING =====\n    if valid_auc > best_valid_auc:\n        best_valid_auc = valid_auc\n        patience_counter = 0\n        torch.save(model.state_dict(), MODEL_PATH)\n        print(f\"ðŸ”¥ New best model saved (AUC = {best_valid_auc:.4f})\")\n    else:\n        patience_counter += 1\n        print(f\"â³ No improvement ({patience_counter}/{patience})\")\n\n    if patience_counter >= patience:\n        print(\"ðŸ›‘ Early stopping triggered\")\n        break","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Experimental Results\n\nThe proposed multimodal CTR model achieves competitive performance on the validation set, demonstrating the effectiveness of incorporating multimodal item embeddings into CTR prediction. The final predictions are submitted to the Codabench platform for official evaluation.\n","metadata":{}},{"cell_type":"code","source":"print(f\"Best Validation AUC: {best_valid_auc:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 5. Test Inference and Submission\n\nIn this section, we load the best trained model and generate predicted click-through rates (pCTR) for the test set. The predictions are formatted according to the Codabench submission requirements.\n","metadata":{}},{"cell_type":"code","source":"# ==========================\n# Load best trained model\n# ==========================\nmodel.load_state_dict(torch.load(MODEL_PATH, map_location=device))\nmodel.eval()\n\ntest_ids = []\ntest_pctr = []\n\nwith torch.no_grad():\n    for batch in tqdm(test_loader, desc=\"Inference on test\"):\n        logits = model(\n            batch[\"user_id\"].to(device),\n            batch[\"item_id\"].to(device),\n            batch[\"item_seq\"].to(device),\n            batch[\"likes\"].to(device),\n            batch[\"views\"].to(device)\n        )\n\n        probs = torch.sigmoid(logits)\n\n        test_ids.extend(batch[\"ID\"].cpu().numpy())\n        test_pctr.extend(probs.cpu().numpy())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==========================\n# Build submission file\n# ==========================\nimport pandas as pd\n\nsubmission_df = pd.DataFrame({\n    \"ID\": test_ids,\n    \"Task1&2\": test_pctr\n})\n\nSUB_PATH = \"/kaggle/working/submission.csv\"\nsubmission_df.to_csv(SUB_PATH, index=False)\n\nprint(\"Submission file saved at:\", SUB_PATH)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"assert len(submission_df) == test_df.shape[0]\nassert submission_df[\"Task1&2\"].between(0, 1).all()\nassert not submission_df.isnull().any().any()\nassert submission_df[\"ID\"].is_unique\n\nprint(\"âœ… Submission sanity check passed\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 7. Conclusion\nWe implemented a complete multimodal CTR prediction pipeline, combining textual and visual item information with user interaction data. The proposed model improves CTR prediction performance and satisfies the low-latency and reproducibility constraints of the MM-CTR Challenge.","metadata":{}}]}