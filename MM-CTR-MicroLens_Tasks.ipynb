{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13875146,"sourceType":"datasetVersion","datasetId":8820441}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Multimodal Click-Through Rate Prediction (MM-CTR)\n\nThis notebook presents an end-to-end multimodal CTR prediction system combining\nuser behavior sequences, categorical interaction features, and precomputed\nmultimodal item embeddings.\n","metadata":{}},{"cell_type":"markdown","source":"## 1. Introduction\n\nClick-Through Rate (CTR) prediction is a core problem in recommender systems,\naiming to estimate the probability that a user will interact with a given item.\nAccurate CTR prediction enables personalized content ranking, advertising\noptimization, and improved user engagement.\n\nIn this project, we address multimodal CTR prediction by integrating:\n- User–item interaction histories,\n- Behavioral feedback signals (likes and views),\n- Sequential browsing patterns,\n- Precomputed multimodal item embeddings.\n\nThe objective is to design, train, and evaluate a deep learning model capable of\ncapturing both sequential user behavior and rich item representations within a\nsingle, reproducible pipeline.\n","metadata":{}},{"cell_type":"code","source":"# Force a compatible protobuf version\n# Some libraries (CLIP, sentence-transformers) may break with newer versions\n!pip uninstall -y protobuf\n!pip install protobuf==3.20.3","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T14:23:08.595554Z","iopub.execute_input":"2025-12-18T14:23:08.596278Z","iopub.status.idle":"2025-12-18T14:23:15.914460Z","shell.execute_reply.started":"2025-12-18T14:23:08.596250Z","shell.execute_reply":"2025-12-18T14:23:15.913342Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Found existing installation: protobuf 6.33.0\nUninstalling protobuf-6.33.0:\n  Successfully uninstalled protobuf-6.33.0\nCollecting protobuf==3.20.3\n  Downloading protobuf-3.20.3-py2.py3-none-any.whl.metadata (720 bytes)\nDownloading protobuf-3.20.3-py2.py3-none-any.whl (162 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: protobuf\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\nopentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 3.20.3 which is incompatible.\nonnx 1.18.0 requires protobuf>=4.25.1, but you have protobuf 3.20.3 which is incompatible.\na2a-sdk 0.3.10 requires protobuf>=5.29.5, but you have protobuf 3.20.3 which is incompatible.\nray 2.51.1 requires click!=8.3.0,>=7.0, but you have click 8.3.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\ntensorflow-metadata 1.17.2 requires protobuf>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.3 which is incompatible.\npydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.3 which is incompatible.\npydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\nydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 3.20.3 which is incompatible.\ngrpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 3.20.3 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.10.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed protobuf-3.20.3\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Install OpenAI CLIP directly from GitHub\n# Kaggle does not provide it by default\n!pip install git+https://github.com/openai/CLIP.git","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T14:24:42.435682Z","iopub.execute_input":"2025-12-18T14:24:42.436699Z","iopub.status.idle":"2025-12-18T14:26:11.745035Z","shell.execute_reply.started":"2025-12-18T14:24:42.436664Z","shell.execute_reply":"2025-12-18T14:26:11.744070Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Collecting git+https://github.com/openai/CLIP.git\n  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-cbl9alrk\n  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-cbl9alrk\n  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting ftfy (from clip==1.0)\n  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (25.0)\nRequirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2025.11.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (4.67.1)\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2.6.0+cu124)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (0.21.0+cu124)\nRequirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->clip==1.0) (0.2.13)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.20.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (4.15.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2025.10.0)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->clip==1.0)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->clip==1.0)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->clip==1.0)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch->clip==1.0)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch->clip==1.0)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch->clip==1.0)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch->clip==1.0)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch->clip==1.0)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch->clip==1.0)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch->clip==1.0)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->clip==1.0) (1.3.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (11.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->clip==1.0) (3.0.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->clip==1.0) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->clip==1.0) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->clip==1.0) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->clip==1.0) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->clip==1.0) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->clip==1.0) (2.4.1)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision->clip==1.0) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision->clip==1.0) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision->clip==1.0) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision->clip==1.0) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision->clip==1.0) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision->clip==1.0) (2024.2.0)\nDownloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m108.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m86.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: clip\n  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369490 sha256=7e7b0abab3152ef5696fb62cdcb0054658fef2bbd76fc4710c4f0b01c9d4cd88\n  Stored in directory: /tmp/pip-ephem-wheel-cache-tf0fg22q/wheels/3f/7c/a4/9b490845988bf7a4db33674d52f709f088f64392063872eb9a\nSuccessfully built clip\nInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, ftfy, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, clip\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed clip-1.0 ftfy-6.3.1 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Additional libraries for training & evaluation\n!pip install -q polars tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T14:28:06.663931Z","iopub.execute_input":"2025-12-18T14:28:06.664678Z","iopub.status.idle":"2025-12-18T14:28:09.797973Z","shell.execute_reply.started":"2025-12-18T14:28:06.664640Z","shell.execute_reply":"2025-12-18T14:28:09.796948Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# ==========================\n# Core imports\n# ==========================\nimport os\nimport warnings\n\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn.functional as F\n\nfrom PIL import Image\nimport clip\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.decomposition import PCA\n\nimport polars as pl\nimport torch.nn as nn\n\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchmetrics.classification import BinaryAUROC\nfrom tqdm import tqdm\n\n\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T14:28:14.059028Z","iopub.execute_input":"2025-12-18T14:28:14.059405Z","iopub.status.idle":"2025-12-18T14:29:08.223822Z","shell.execute_reply.started":"2025-12-18T14:28:14.059377Z","shell.execute_reply":"2025-12-18T14:29:08.223025Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stderr","text":"2025-12-18 14:28:33.926869: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1766068114.321581      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1766068114.450550      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"}],"execution_count":3},{"cell_type":"markdown","source":"# 2. Dataset Overview\n\nWe use the MicroLens-1M dataset, which contains user click interactions and rich item content features such as titles and cover images.\n\nThe dataset is split into training, validation, and test sets. The training and\nvalidation sets are used for model optimization and hyperparameter tuning, while\nthe test set is reserved for final inference and submission generation.\n","metadata":{}},{"cell_type":"code","source":"# ==========================\n# Dataset paths (Kaggle)\n# ==========================\nINFO_FILE = \"/kaggle/input/item_info.parquet\"\nFEATURE_FILE = \"/kaggle/input/item_feature.parquet\"\nIMAGE_DIR = \"/kaggle/input/item_images/item_images/\"\n\nSAVE_PATH = \"/kaggle/working/item_info_multimodal_emb.parquet\"\n\n# ==========================\n# Read parquet files\n# ==========================\ndf_info = pd.read_parquet(INFO_FILE)\ndf_features = pd.read_parquet(FEATURE_FILE)\n\nprint(\"Item info shape :\", df_info.shape)\nprint(\"Item feature shape :\", df_features.shape)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-18T14:29:15.323670Z","iopub.execute_input":"2025-12-18T14:29:15.324707Z","iopub.status.idle":"2025-12-18T14:29:27.671896Z","shell.execute_reply.started":"2025-12-18T14:29:15.324681Z","shell.execute_reply":"2025-12-18T14:29:27.671246Z"}},"outputs":[{"name":"stdout","text":"Item info shape : (91718, 3)\nItem feature shape : (91717, 7)\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# 3. Task 1: Multimodal Item Embedding\nIn this section, we extract textual and visual representations for each item, then fuse them into a compact 128-dimensional embedding.\n","metadata":{}},{"cell_type":"markdown","source":"# 3.1 Text Embedding and Image Embedding\n\nWe encode item textual information using a pretrained sentence-level encoder to obtain dense semantic representations: Sentence-BERT\n\nItem cover images are processed using a pretrained vision model to extract visual embeddings: CLIP-ResNet50","metadata":{}},{"cell_type":"code","source":"# ==========================\n# Device configuration\n# ==========================\ncompute_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# ==========================\n# Text encoder (Sentence-BERT)\n# ==========================\ntext_encoder = SentenceTransformer(\"all-MiniLM-L6-v2\")\ntext_encoder.eval()\n\n# ==========================\n# Image encoder (CLIP ResNet-50)\n# ==========================\nclip_net, clip_preprocess = clip.load(\"RN50\", device=compute_device)\nclip_net.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T14:29:32.043022Z","iopub.execute_input":"2025-12-18T14:29:32.043418Z","iopub.status.idle":"2025-12-18T14:29:46.932435Z","shell.execute_reply.started":"2025-12-18T14:29:32.043392Z","shell.execute_reply":"2025-12-18T14:29:46.931683Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"898393c1658e4c39af1ab9540f3e754e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"514c1b2fe6cc4994b420f3657443a036"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"833af7a6783d4d658052ce11c94e015f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7050e249db445f7beaeecfad6bcc3f2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f167049cb5184c369eee8ad557a59331"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41706188123946c19fcc2181595b8de3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9229628686904736b8e103f5488dc36b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0dc4802e6b0a4cb8bbfb77b11fabff5a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1fa2126dbf6a4a19907dfdaad27a7648"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d87834f47c1446a8a001db44b651e448"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7c4aacf93a7421991c1839192ec1a48"}},"metadata":{}},{"name":"stderr","text":"100%|████████████████████████████████████████| 244M/244M [00:01<00:00, 130MiB/s]\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"CLIP(\n  (visual): ModifiedResNet(\n    (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n    (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (relu1): ReLU(inplace=True)\n    (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (relu2): ReLU(inplace=True)\n    (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (relu3): ReLU(inplace=True)\n    (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n    (layer1): Sequential(\n      (0): Bottleneck(\n        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (avgpool): Identity()\n        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu3): ReLU(inplace=True)\n        (downsample): Sequential(\n          (-1): AvgPool2d(kernel_size=1, stride=1, padding=0)\n          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): Bottleneck(\n        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (avgpool): Identity()\n        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu3): ReLU(inplace=True)\n      )\n      (2): Bottleneck(\n        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (avgpool): Identity()\n        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu3): ReLU(inplace=True)\n      )\n    )\n    (layer2): Sequential(\n      (0): Bottleneck(\n        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu3): ReLU(inplace=True)\n        (downsample): Sequential(\n          (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)\n          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): Bottleneck(\n        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (avgpool): Identity()\n        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu3): ReLU(inplace=True)\n      )\n      (2): Bottleneck(\n        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (avgpool): Identity()\n        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu3): ReLU(inplace=True)\n      )\n      (3): Bottleneck(\n        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (avgpool): Identity()\n        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu3): ReLU(inplace=True)\n      )\n    )\n    (layer3): Sequential(\n      (0): Bottleneck(\n        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu3): ReLU(inplace=True)\n        (downsample): Sequential(\n          (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)\n          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): Bottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (avgpool): Identity()\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu3): ReLU(inplace=True)\n      )\n      (2): Bottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (avgpool): Identity()\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu3): ReLU(inplace=True)\n      )\n      (3): Bottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (avgpool): Identity()\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu3): ReLU(inplace=True)\n      )\n      (4): Bottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (avgpool): Identity()\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu3): ReLU(inplace=True)\n      )\n      (5): Bottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (avgpool): Identity()\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu3): ReLU(inplace=True)\n      )\n    )\n    (layer4): Sequential(\n      (0): Bottleneck(\n        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu3): ReLU(inplace=True)\n        (downsample): Sequential(\n          (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)\n          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): Bottleneck(\n        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (avgpool): Identity()\n        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu3): ReLU(inplace=True)\n      )\n      (2): Bottleneck(\n        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (avgpool): Identity()\n        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu3): ReLU(inplace=True)\n      )\n    )\n    (attnpool): AttentionPool2d(\n      (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n      (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n      (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n      (c_proj): Linear(in_features=2048, out_features=1024, bias=True)\n    )\n  )\n  (transformer): Transformer(\n    (resblocks): Sequential(\n      (0): ResidualAttentionBlock(\n        (attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (mlp): Sequential(\n          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n          (gelu): QuickGELU()\n          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n        )\n        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      )\n      (1): ResidualAttentionBlock(\n        (attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (mlp): Sequential(\n          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n          (gelu): QuickGELU()\n          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n        )\n        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      )\n      (2): ResidualAttentionBlock(\n        (attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (mlp): Sequential(\n          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n          (gelu): QuickGELU()\n          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n        )\n        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      )\n      (3): ResidualAttentionBlock(\n        (attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (mlp): Sequential(\n          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n          (gelu): QuickGELU()\n          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n        )\n        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      )\n      (4): ResidualAttentionBlock(\n        (attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (mlp): Sequential(\n          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n          (gelu): QuickGELU()\n          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n        )\n        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      )\n      (5): ResidualAttentionBlock(\n        (attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (mlp): Sequential(\n          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n          (gelu): QuickGELU()\n          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n        )\n        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      )\n      (6): ResidualAttentionBlock(\n        (attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (mlp): Sequential(\n          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n          (gelu): QuickGELU()\n          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n        )\n        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      )\n      (7): ResidualAttentionBlock(\n        (attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (mlp): Sequential(\n          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n          (gelu): QuickGELU()\n          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n        )\n        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      )\n      (8): ResidualAttentionBlock(\n        (attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (mlp): Sequential(\n          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n          (gelu): QuickGELU()\n          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n        )\n        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      )\n      (9): ResidualAttentionBlock(\n        (attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (mlp): Sequential(\n          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n          (gelu): QuickGELU()\n          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n        )\n        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      )\n      (10): ResidualAttentionBlock(\n        (attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (mlp): Sequential(\n          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n          (gelu): QuickGELU()\n          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n        )\n        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      )\n      (11): ResidualAttentionBlock(\n        (attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (mlp): Sequential(\n          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n          (gelu): QuickGELU()\n          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n        )\n        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      )\n    )\n  )\n  (token_embedding): Embedding(49408, 512)\n  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n)"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"# ==========================\n# Text embeddings\n# ==========================\nitem_titles = df_features[\"item_title\"].tolist()\n\ntext_vectors = text_encoder.encode(\n    item_titles,\n    batch_size=256,\n    show_progress_bar=True\n)\n\n# ==========================\n# Image embeddings \n# ==========================\ndef extract_image_embedding(image_path):\n    \"\"\"\n    Compute CLIP embedding for a single image.\n    If the image is missing or corrupted, return a zero vector.\n    \"\"\"\n    try:\n        img = Image.open(image_path).convert(\"RGB\")\n        img_tensor = clip_preprocess(img).unsqueeze(0).to(compute_device)\n\n        with torch.no_grad():\n            img_emb = clip_net.encode_image(img_tensor)\n\n        return img_emb.cpu().numpy().squeeze()\n\n    except Exception:\n        return np.zeros(clip_net.visual.output_dim)\n\n\n# ==========================\n# Encode all item images\n# ==========================\nimage_vectors = []\n\nfor iid in df_features[\"item_id\"]:\n    img_path = os.path.join(IMAGE_DIR, f\"image{iid}.jpg\")\n    image_vectors.append(extract_image_embedding(img_path))\n\nimage_vectors = np.array(image_vectors)\n\nprint(\"Image embedding matrix shape:\", image_vectors.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T14:29:57.883071Z","iopub.execute_input":"2025-12-18T14:29:57.883848Z","iopub.status.idle":"2025-12-18T14:32:40.008266Z","shell.execute_reply.started":"2025-12-18T14:29:57.883822Z","shell.execute_reply":"2025-12-18T14:32:40.007442Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/359 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40ab0c7911214edf8bae990fc16aeff4"}},"metadata":{}},{"name":"stdout","text":"Image embedding matrix shape: (91717, 1024)\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Inspect available columns\nprint(df_info.columns.tolist())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T14:33:11.137005Z","iopub.execute_input":"2025-12-18T14:33:11.137368Z","iopub.status.idle":"2025-12-18T14:33:11.141665Z","shell.execute_reply.started":"2025-12-18T14:33:11.137340Z","shell.execute_reply":"2025-12-18T14:33:11.140941Z"}},"outputs":[{"name":"stdout","text":"['item_id', 'item_tags', 'item_emb_d128']\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"# 3.3 Multimodal Fusion and Dimensionality Reduction\n\nText and image embeddings are concatenated to form multimodal representations. To satisfy competition constraints and reduce inference latency, PCA is applied to project embeddings to 128 dimensions.","metadata":{}},{"cell_type":"code","source":"from IPython.display import FileLink\n\n# ==========================\n# Multimodal fusion (text + image)\n# ==========================\nmultimodal_features = np.concatenate(\n    [text_vectors, image_vectors],\n    axis=1\n)\n\nprint(\"Fused embedding shape:\", multimodal_features.shape)\n\n# ==========================\n# Padding first row (alignment with item_info)\n# ==========================\nzero_padding = np.zeros((1, multimodal_features.shape[1]))\nmultimodal_features_padded = np.vstack(\n    [zero_padding, multimodal_features]\n)\n\nprint(\"After padding:\", multimodal_features_padded.shape)\n\n# ==========================\n# Dimensionality reduction (PCA → 128)\n# ==========================\npca_model = PCA(n_components=128)\nmultimodal_128d = pca_model.fit_transform(multimodal_features_padded)\n\nprint(\"Final embedding shape:\", multimodal_128d.shape)\n\n# ==========================\n# Save into item_info\n# ==========================\ndf_info[\"item_emb_d128\"] = list(multimodal_128d)\ndf_info.to_parquet(SAVE_PATH, index=False)\n#You can download the saved file from /kaggle/working/ directory\nprint(f\"Saved fused embeddings to: {SAVE_PATH}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T14:33:17.055755Z","iopub.execute_input":"2025-12-18T14:33:17.056399Z","iopub.status.idle":"2025-12-18T14:33:30.414142Z","shell.execute_reply.started":"2025-12-18T14:33:17.056372Z","shell.execute_reply":"2025-12-18T14:33:30.413250Z"}},"outputs":[{"name":"stdout","text":"Fused embedding shape: (91717, 1408)\nAfter padding: (91718, 1408)\nFinal embedding shape: (91718, 128)\nSaved fused embeddings to: /kaggle/working/item_info_multimodal_emb.parquet\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# Reload saved file for sanity check\ndf_check = pd.read_parquet(SAVE_PATH)\n\nprint(df_check.head())\nprint(\"Embedding dimension example:\", len(df_check[\"item_emb_d128\"].iloc[0]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T14:33:34.103088Z","iopub.execute_input":"2025-12-18T14:33:34.103650Z","iopub.status.idle":"2025-12-18T14:33:34.606957Z","shell.execute_reply.started":"2025-12-18T14:33:34.103624Z","shell.execute_reply":"2025-12-18T14:33:34.606072Z"}},"outputs":[{"name":"stdout","text":"   item_id        item_tags                                      item_emb_d128\n0        0  [0, 0, 0, 0, 0]  [-0.09303953632827579, -0.023277998817579422, ...\n1        1  [0, 0, 0, 0, 1]  [-0.012815042967732978, -0.09739979676869429, ...\n2        2  [0, 0, 2, 3, 4]  [-0.04312385184114877, 0.007002070534522362, -...\n3        3  [0, 0, 5, 6, 7]  [0.11733471759868354, -0.07496687006709006, -0...\n4        4  [0, 0, 0, 8, 9]  [0.02941412000516612, -0.02858132508637742, -0...\nEmbedding dimension example: 128\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"# 4. Task 2: Multimodal CTR Prediction\n\nIn this task, we focus on predicting the click-through rate (CTR) by effectively leveraging the multimodal item embeddings generated in Task 1. The objective is to design an end-to-end neural CTR model that integrates user interaction features with multimodal item representations.\n\nThe model takes as input user identifiers, target items, historical interaction sequences, and engagement signals (likes and views), together with the 128-dimensional multimodal item embeddings. These features are jointly learned through embedding layers and a multilayer perceptron (MLP) to estimate the probability of a user clicking on a given item.\n\nThe model is trained using binary cross-entropy loss and evaluated using the Area Under the ROC Curve (AUC), following the official data split and competition constraints.\n","metadata":{}},{"cell_type":"markdown","source":"# 4.1 Problem Formulation\n\nGiven user interaction history and item multimodal embeddings, the task is to predict the probability of a click (pCTR; I named it Task1&2 in the submission.csv as you'll find at the end of this notebook) for each user–item pair.","metadata":{}},{"cell_type":"code","source":"# ==========================\n# Kaggle dataset paths\n# ==========================\nBASE_PATH = \"/kaggle/input/\"  \n\ntrain_df = pl.read_parquet(f\"{BASE_PATH}/train.parquet\")\nvalid_df = pl.read_parquet(f\"{BASE_PATH}/valid.parquet\")\ntest_df  = pl.read_parquet(f\"{BASE_PATH}/test.parquet\")\n\n# Load item_info WITH multimodal embeddings\nitem_info = pl.read_parquet(\n    \"/kaggle/working/item_info_multimodal_emb.parquet\"\n)\n\nprint(\n    \"Train / Valid / Test shapes:\",\n    train_df.shape, valid_df.shape, test_df.shape\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T14:33:48.223263Z","iopub.execute_input":"2025-12-18T14:33:48.223734Z","iopub.status.idle":"2025-12-18T14:33:52.940463Z","shell.execute_reply.started":"2025-12-18T14:33:48.223711Z","shell.execute_reply":"2025-12-18T14:33:52.939727Z"}},"outputs":[{"name":"stdout","text":"Train / Valid / Test shapes: (3600000, 6) (10000, 6) (379142, 6)\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# ==========================\n# Device configuration\n# ==========================\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Using device:\", device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T14:34:10.303063Z","iopub.execute_input":"2025-12-18T14:34:10.303679Z","iopub.status.idle":"2025-12-18T14:34:10.307934Z","shell.execute_reply.started":"2025-12-18T14:34:10.303654Z","shell.execute_reply":"2025-12-18T14:34:10.307253Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"### Feature Engineering\n\nIn addition to raw identifiers, we engineer auxiliary features to improve model\ngeneralization. In particular, item popularity is computed from the training\ndata and log-scaled to reduce skewness. This feature captures global item\nexposure patterns and complements personalized user representations.\n","metadata":{}},{"cell_type":"code","source":"# ==========================\n# Item Embeddings + Feature Engineering\n# ==========================\nitem_emb_matrix = np.stack(\n    item_info[\"item_emb_d128\"].to_numpy()\n)\n\nitem_emb_tensor = torch.tensor(\n    item_emb_matrix,\n    dtype=torch.float32,\n    device=device\n)\n\nnum_items = item_emb_tensor.shape[0]\nprint(\"Number of items (including padding):\", num_items)\n\n# Compute item popularity for better features\nitem_popularity = train_df.group_by(\"item_id\").agg(\n    pl.count(\"user_id\").alias(\"item_count\")\n).sort(\"item_id\")\n\n# Create popularity tensor (log-scaled)\nitem_pop_dict = dict(zip(\n    item_popularity[\"item_id\"].to_list(),\n    item_popularity[\"item_count\"].to_list()\n))\n\nitem_pop_array = np.array([\n    np.log1p(item_pop_dict.get(i, 0)) for i in range(num_items)\n])\n\nitem_popularity_tensor = torch.tensor(\n    item_pop_array,\n    dtype=torch.float32,\n    device=device\n)\n\nprint(\"Item popularity features computed\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T14:34:20.062823Z","iopub.execute_input":"2025-12-18T14:34:20.063136Z","iopub.status.idle":"2025-12-18T14:34:22.015951Z","shell.execute_reply.started":"2025-12-18T14:34:20.063113Z","shell.execute_reply":"2025-12-18T14:34:22.015241Z"}},"outputs":[{"name":"stdout","text":"Number of items (including padding): 91718\nItem popularity features computed\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"# 4.2 Model Architecture\n\nWe design an end-to-end neural CTR model that ingests user features and item multimodal embeddings. The model outputs a scalar pCTR via fully connected layers.","metadata":{}},{"cell_type":"markdown","source":"The custom dataset class handles data preparation for both training and\ninference. It supports variable-length interaction sequences and explicitly\ncomputes sequence length as an additional numerical feature.","metadata":{}},{"cell_type":"code","source":"# ==========================\n# CTR Dataset\n# ==========================\nclass CTRDataset(Dataset):\n\n    def __init__(self, df, is_test=False):\n        self.is_test = is_test\n\n        self.user_id  = df[\"user_id\"].to_numpy()\n        self.item_id  = df[\"item_id\"].to_numpy()\n        self.item_seq = df[\"item_seq\"].to_numpy()\n        self.likes    = df[\"likes_level\"].to_numpy()\n        self.views    = df[\"views_level\"].to_numpy()\n\n        # Compute sequence length as feature\n        self.seq_len = np.array([\n            np.count_nonzero(seq) for seq in self.item_seq\n        ])\n\n        if not is_test:\n            self.label = df[\"label\"].to_numpy()\n        else:\n            self.ID = df[\"ID\"].to_numpy()\n\n    def __len__(self):\n        return len(self.user_id)\n\n    def __getitem__(self, idx):\n        sample = {\n            \"user_id\": torch.tensor(self.user_id[idx], dtype=torch.long),\n            \"item_id\": torch.tensor(self.item_id[idx], dtype=torch.long),\n            \"item_seq\": torch.tensor(self.item_seq[idx], dtype=torch.long),\n            \"likes\": torch.tensor(self.likes[idx], dtype=torch.long),\n            \"views\": torch.tensor(self.views[idx], dtype=torch.long),\n            \"seq_len\": torch.tensor(self.seq_len[idx], dtype=torch.float32),  # 🆕 NEW\n        }\n\n        if self.is_test:\n            sample[\"ID\"] = torch.tensor(self.ID[idx], dtype=torch.long)\n        else:\n            sample[\"label\"] = torch.tensor(self.label[idx], dtype=torch.float32)\n\n        return sample","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T14:34:29.703487Z","iopub.execute_input":"2025-12-18T14:34:29.704057Z","iopub.status.idle":"2025-12-18T14:34:29.711567Z","shell.execute_reply.started":"2025-12-18T14:34:29.704032Z","shell.execute_reply":"2025-12-18T14:34:29.710892Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# ==========================\n# DataLoaders \n# ==========================\nBATCH_SIZE = 1024\n\ntrain_loader = DataLoader(\n    CTRDataset(train_df),\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    num_workers=0,      # IMPORTANT for Kaggle users\n    pin_memory=False    # IMPORTANT for Kaggle users\n)\n\nvalid_loader = DataLoader(\n    CTRDataset(valid_df),\n    batch_size=BATCH_SIZE,\n    shuffle=False,\n    num_workers=0,\n    pin_memory=False\n)\n\ntest_loader = DataLoader(\n    CTRDataset(test_df, is_test=True),\n    batch_size=BATCH_SIZE,\n    shuffle=False,\n    num_workers=0,\n    pin_memory=False\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T14:34:35.533368Z","iopub.execute_input":"2025-12-18T14:34:35.533938Z","iopub.status.idle":"2025-12-18T14:34:39.064138Z","shell.execute_reply.started":"2025-12-18T14:34:35.533917Z","shell.execute_reply":"2025-12-18T14:34:39.063469Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"## Model Architecture\n\nWe propose an enhanced multimodal CTR prediction model that combines embedding\nlayers, attention-based sequence modeling, feature interaction mechanisms, and a\ndeep multilayer perceptron (MLP).\n","metadata":{}},{"cell_type":"markdown","source":"Multi-head attention is used to aggregate sequential item interactions, enabling\nthe model to focus on the most relevant past behaviors. Multimodal item\nembeddings are projected into the same latent space as categorical features to\nallow effective fusion.","metadata":{}},{"cell_type":"code","source":"# ==========================\n# ENHANCED MMCTR Model\n# ==========================\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, emb_dim, num_heads=4):\n        super().__init__()\n        self.num_heads = num_heads\n        self.head_dim = emb_dim // num_heads\n        \n        self.qkv = nn.Linear(emb_dim, emb_dim * 3)\n        self.out = nn.Linear(emb_dim, emb_dim)\n        \n    def forward(self, x, mask=None):\n        B, L, D = x.shape\n        \n        qkv = self.qkv(x).reshape(B, L, 3, self.num_heads, self.head_dim)\n        qkv = qkv.permute(2, 0, 3, 1, 4)\n        q, k, v = qkv[0], qkv[1], qkv[2]\n        \n        attn = (q @ k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n        \n        if mask is not None:\n            attn = attn.masked_fill(mask.unsqueeze(1).unsqueeze(2) == 0, -1e9)\n        \n        attn = F.softmax(attn, dim=-1)\n        out = (attn @ v).transpose(1, 2).reshape(B, L, D)\n        \n        return self.out(out)\n\n\nclass EnhancedMMCTRModel(nn.Module):\n    def __init__(self, num_users, num_items, emb_dim=64):  \n        super().__init__()\n\n        # Embeddings (larger capacity)\n        self.user_emb = nn.Embedding(num_users + 1, emb_dim, padding_idx=0)\n        self.item_emb = nn.Embedding(num_items, emb_dim, padding_idx=0)\n        self.likes_emb = nn.Embedding(11, emb_dim)\n        self.views_emb = nn.Embedding(11, emb_dim)\n\n        # Multi-head attention for sequence\n        self.attention = MultiHeadAttention(emb_dim, num_heads=4)\n        \n        # Transform multimodal embeddings\n        self.mm_transform = nn.Sequential(\n            nn.Linear(128, emb_dim),\n            nn.ReLU(),\n            nn.Dropout(0.2)\n        )\n\n        # Feature interaction layer (FM-style)\n        self.feature_interaction = nn.Bilinear(emb_dim, emb_dim, emb_dim)\n\n        # Deeper MLP with residual connections and BatchNorm\n        self.bn1 = nn.BatchNorm1d(emb_dim * 6 + 1)  # +1 for seq_len\n        \n        self.mlp1 = nn.Sequential(\n            nn.Linear(emb_dim * 6 + 1, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(),\n            nn.Dropout(0.3)\n        )\n        \n        self.mlp2 = nn.Sequential(\n            nn.Linear(512, 256),\n            nn.BatchNorm1d(256),\n            nn.ReLU(),\n            nn.Dropout(0.2)\n        )\n        \n        self.mlp3 = nn.Sequential(\n            nn.Linear(256, 128),\n            nn.BatchNorm1d(128),\n            nn.ReLU(),\n            nn.Dropout(0.1)\n        )\n        \n        self.output = nn.Linear(128, 1)\n\n        # Initialize weights properly\n        self._init_weights()\n\n    def _init_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Linear):\n                nn.init.xavier_uniform_(m.weight)\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Embedding):\n                nn.init.normal_(m.weight, mean=0, std=0.01)\n\n    def forward(self, user_id, item_id, item_seq, likes, views, seq_len):\n        # Basic embeddings\n        user_e = self.user_emb(user_id)\n        item_e = self.item_emb(item_id)\n        likes_e = self.likes_emb(likes)\n        views_e = self.views_emb(views)\n\n        # Attention-based sequence pooling\n        seq_e = self.item_emb(item_seq)\n        mask = (item_seq != 0)\n        \n        # Apply attention\n        seq_attended = self.attention(seq_e, mask)\n        \n        # Pool with attention weights\n        seq_pooled = (seq_attended * mask.unsqueeze(-1)).sum(1) / (mask.sum(1, keepdim=True) + 1e-8)\n\n        # Multimodal item embedding (transformed)\n        item_mm_e = item_emb_tensor[item_id]\n        item_mm_e = self.mm_transform(item_mm_e)\n\n        # Feature interaction (user-item, item-sequence)\n        ui_interaction = self.feature_interaction(user_e, item_e)\n\n        # Normalize sequence length\n        seq_len_norm = (seq_len.unsqueeze(1) / 50.0).clamp(0, 1)\n\n        # Concatenate all features\n        x = torch.cat(\n            [user_e, item_e, seq_pooled, likes_e, views_e, item_mm_e, seq_len_norm],\n            dim=1\n        )\n\n        # BatchNorm before MLP\n        x = self.bn1(x)\n\n        # Deep MLP with residual-style connections\n        x1 = self.mlp1(x)\n        x2 = self.mlp2(x1)\n        x3 = self.mlp3(x2)\n\n        return self.output(x3).squeeze(1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T14:34:52.571089Z","iopub.execute_input":"2025-12-18T14:34:52.571821Z","iopub.status.idle":"2025-12-18T14:34:52.586020Z","shell.execute_reply.started":"2025-12-18T14:34:52.571795Z","shell.execute_reply":"2025-12-18T14:34:52.585091Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# ==========================\n#  Model Setup\n# ==========================\nnum_users = int(train_df[\"user_id\"].max())\n\nmodel = EnhancedMMCTRModel(\n    num_users=num_users,\n    num_items=num_items,\n    emb_dim=64  \n).to(device)\n\n# Label smoothing for better generalization\nclass BCEWithLogitsLossSmoothed(nn.Module):\n    def __init__(self, smoothing=0.1):\n        super().__init__()\n        self.smoothing = smoothing\n        self.bce = nn.BCEWithLogitsLoss()\n    \n    def forward(self, logits, targets):\n        targets = targets * (1 - self.smoothing) + 0.5 * self.smoothing\n        return self.bce(logits, targets)\n\ncriterion = BCEWithLogitsLossSmoothed(smoothing=0.05)\n\n# AdamW with weight decay for better regularization\noptimizer = torch.optim.AdamW(\n    model.parameters(),\n    lr=1e-3,\n    weight_decay=1e-5\n)\n\n# Learning rate scheduler\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer,\n    mode='max',\n    factor=0.5,\n    patience=2,\n    verbose=True\n)\n\nauc_metric = BinaryAUROC().to(device)\n\nprint(f\"✅ Model initialized with {sum(p.numel() for p in model.parameters()):,} parameters\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T14:34:54.455407Z","iopub.execute_input":"2025-12-18T14:34:54.456053Z","iopub.status.idle":"2025-12-18T14:34:55.406815Z","shell.execute_reply.started":"2025-12-18T14:34:54.456030Z","shell.execute_reply":"2025-12-18T14:34:55.406219Z"}},"outputs":[{"name":"stdout","text":"✅ Model initialized with 70,523,075 parameters\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"# 4.3 Model Training and Validation\n\nThe multimodal CTR model is trained on the training set and evaluated on the validation set using the Area Under the ROC Curve (AUC), a standard metric for imbalanced binary classification. Training uses binary cross-entropy loss with label smoothing to improve generalization and the AdamW optimizer with weight decay to mitigate overfitting. Gradient clipping is applied to stabilize optimization, while early stopping based on validation AUC ensures optimal model selection for final inference.\n","metadata":{}},{"cell_type":"code","source":"# ==========================\n# Training Loop\n# ==========================\nEPOCHS = 30  \nbest_valid_auc = 0.0\npatience = 5  \npatience_counter = 0\n\nMODEL_PATH = \"/kaggle/working/best_mmctr_model.pt\"\n\nfor epoch in range(EPOCHS):\n\n    # ===== TRAIN =====\n    model.train()\n    epoch_loss = 0.0\n\n    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n        optimizer.zero_grad()\n\n        logits = model(\n            batch[\"user_id\"].to(device),\n            batch[\"item_id\"].to(device),\n            batch[\"item_seq\"].to(device),\n            batch[\"likes\"].to(device),\n            batch[\"views\"].to(device),\n            batch[\"seq_len\"].to(device) \n        )\n\n        loss = criterion(logits, batch[\"label\"].to(device))\n        loss.backward()\n        \n        # Gradient clipping for stability\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        \n        optimizer.step()\n\n        epoch_loss += loss.item()\n\n    avg_train_loss = epoch_loss / len(train_loader)\n    print(f\"Epoch {epoch+1} | Train Loss: {avg_train_loss:.4f}\")\n\n    # ===== VALID =====\n    model.eval()\n    auc_metric.reset()\n\n    with torch.no_grad():\n        for batch in valid_loader:\n            logits = model(\n                batch[\"user_id\"].to(device),\n                batch[\"item_id\"].to(device),\n                batch[\"item_seq\"].to(device),\n                batch[\"likes\"].to(device),\n                batch[\"views\"].to(device),\n                batch[\"seq_len\"].to(device)  \n            )\n            probs = torch.sigmoid(logits)\n            auc_metric.update(probs, batch[\"label\"].to(device))\n\n    valid_auc = auc_metric.compute().item()\n    print(f\"Epoch {epoch+1} | Valid AUC: {valid_auc:.4f}\")\n\n    # Learning rate scheduling\n    scheduler.step(valid_auc)\n    current_lr = optimizer.param_groups[0]['lr']\n    print(f\"Current LR: {current_lr:.6f}\")\n\n    # ===== EARLY STOPPING =====\n    if valid_auc > best_valid_auc:\n        best_valid_auc = valid_auc\n        patience_counter = 0\n        torch.save(model.state_dict(), MODEL_PATH)\n        print(f\"🔥 New best model saved (AUC = {best_valid_auc:.4f})\")\n    else:\n        patience_counter += 1\n        print(f\"⏳ No improvement ({patience_counter}/{patience})\")\n\n    if patience_counter >= patience:\n        print(\"🛑 Early stopping triggered\")\n        break\n\nprint(f\"\\n✅ Training complete! Best Valid AUC: {best_valid_auc:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T14:34:57.224229Z","iopub.execute_input":"2025-12-18T14:34:57.224864Z","iopub.status.idle":"2025-12-18T15:16:45.896787Z","shell.execute_reply.started":"2025-12-18T14:34:57.224842Z","shell.execute_reply":"2025-12-18T15:16:45.896074Z"}},"outputs":[{"name":"stderr","text":"Epoch 1: 100%|██████████| 3516/3516 [05:56<00:00,  9.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 | Train Loss: 0.2221\nEpoch 1 | Valid AUC: 0.9135\nCurrent LR: 0.001000\n🔥 New best model saved (AUC = 0.9135)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2: 100%|██████████| 3516/3516 [05:54<00:00,  9.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 | Train Loss: 0.1500\nEpoch 2 | Valid AUC: 0.9146\nCurrent LR: 0.001000\n🔥 New best model saved (AUC = 0.9146)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3: 100%|██████████| 3516/3516 [05:55<00:00,  9.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 | Train Loss: 0.1330\nEpoch 3 | Valid AUC: 0.8933\nCurrent LR: 0.001000\n⏳ No improvement (1/5)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4: 100%|██████████| 3516/3516 [05:56<00:00,  9.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 | Train Loss: 0.1253\nEpoch 4 | Valid AUC: 0.9013\nCurrent LR: 0.001000\n⏳ No improvement (2/5)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5: 100%|██████████| 3516/3516 [05:55<00:00,  9.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5 | Train Loss: 0.1230\nEpoch 5 | Valid AUC: 0.8781\nCurrent LR: 0.000500\n⏳ No improvement (3/5)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6: 100%|██████████| 3516/3516 [05:57<00:00,  9.84it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6 | Train Loss: 0.1200\nEpoch 6 | Valid AUC: 0.8994\nCurrent LR: 0.000500\n⏳ No improvement (4/5)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7: 100%|██████████| 3516/3516 [06:07<00:00,  9.57it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7 | Train Loss: 0.1184\nEpoch 7 | Valid AUC: 0.8858\nCurrent LR: 0.000500\n⏳ No improvement (5/5)\n🛑 Early stopping triggered\n\n✅ Training complete! Best Valid AUC: 0.9146\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"# 5. Test Inference and Submission\n\nDuring inference, the best-performing model checkpoint is loaded and applied to\nthe test set. Predicted probabilities are exported in the required submission\nformat.\n","metadata":{}},{"cell_type":"code","source":"# ==========================\n# Load best trained model\n# ==========================\nmodel.load_state_dict(torch.load(MODEL_PATH, map_location=device))\nmodel.eval()\n\ntest_ids = []\ntest_pctr = []\n\nwith torch.no_grad():\n    for batch in tqdm(test_loader, desc=\"Inference on test\"):\n        logits = model(\n            batch[\"user_id\"].to(device),\n            batch[\"item_id\"].to(device),\n            batch[\"item_seq\"].to(device),\n            batch[\"likes\"].to(device),\n            batch[\"views\"].to(device),\n            batch[\"seq_len\"].to(device)   \n        )\n\n        probs = torch.sigmoid(logits)\n\n        test_ids.extend(batch[\"ID\"].cpu().numpy())\n        test_pctr.extend(probs.cpu().numpy())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T15:16:55.314666Z","iopub.execute_input":"2025-12-18T15:16:55.314954Z","iopub.status.idle":"2025-12-18T15:17:19.149306Z","shell.execute_reply.started":"2025-12-18T15:16:55.314932Z","shell.execute_reply":"2025-12-18T15:17:19.148523Z"}},"outputs":[{"name":"stderr","text":"Inference on test: 100%|██████████| 371/371 [00:23<00:00, 15.74it/s]\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# ==========================\n# Build submission file\n# ==========================\nimport pandas as pd\n\nsubmission_df = pd.DataFrame({\n    \"ID\": test_ids,\n    \"Task1&2\": test_pctr\n})\n\nSUB_PATH = \"/kaggle/working/submission.csv\"\nsubmission_df.to_csv(SUB_PATH, index=False)\n\nprint(\"Submission file saved at:\", SUB_PATH)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T15:19:22.604820Z","iopub.execute_input":"2025-12-18T15:19:22.605714Z","iopub.status.idle":"2025-12-18T15:19:23.898468Z","shell.execute_reply.started":"2025-12-18T15:19:22.605679Z","shell.execute_reply":"2025-12-18T15:19:23.897758Z"}},"outputs":[{"name":"stdout","text":"Submission file saved at: /kaggle/working/submission.csv\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"assert len(submission_df) == test_df.shape[0]\nassert submission_df[\"Task1&2\"].between(0, 1).all()\nassert not submission_df.isnull().any().any()\nassert submission_df[\"ID\"].is_unique\n\nprint(\"✅ Submission sanity check passed\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T15:19:27.702180Z","iopub.execute_input":"2025-12-18T15:19:27.702757Z","iopub.status.idle":"2025-12-18T15:19:27.729425Z","shell.execute_reply.started":"2025-12-18T15:19:27.702734Z","shell.execute_reply":"2025-12-18T15:19:27.728646Z"}},"outputs":[{"name":"stdout","text":"✅ Submission sanity check passed\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"## 7. Conclusion\nWe implemented a complete multimodal CTR prediction pipeline, combining textual and visual item information with user interaction data. The proposed model improves CTR prediction performance and satisfies the low-latency and reproducibility constraints of the MM-CTR Challenge.","metadata":{}}]}